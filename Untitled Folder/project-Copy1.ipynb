{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS236605: Deep Learning\n",
    "\n",
    "## Final project - Glassman Yair, Vanelli Martina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project is based on the article [Adversarial Attacks on Neural Networks for Graph Data](https://arxiv.org/pdf/1805.07984.pdf) (Code: [code](https://github.com/danielzuegner/nettack)) where the authors generated adversarial attacks on attributed graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors attacked a semi-supervised node classification task in a single large graph having binary node features. Formally, given $G=(A,X)$ attributed graph with $A\\in \\{0,1\\}^{N×N}$ representing the connections (adjacency matrix) and $X\\in \\{0,1\\}^{N×D}$ representing the nodes’ features and a subset $\\mathcal{V}_L\\subseteq \\mathcal{V}=\\{1,\\dots,n\\}$ of labeled nodes with class labels from $C=\\{1,2,...,c_K\\}$, the aim of node classification is to learn a function $g:\\mathcal{V}\\rightarrow C$ that labels each node to one class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors focused on node classification employing graph convolution layers. In particular, they considered the model proposed in [Semi-supervised classification with graph convolutional networks](https://arxiv.org/pdf/1609.02907.pdf) and they built a GCN with a single hidden layer with the following structure:\n",
    "$$\n",
    "Z=f_0(A,X)=\\text{softmax}(\\hat{A}\\sigma(\\hat{A}XW^{(1)})W^{(2)})\n",
    "$$\n",
    "where $\\hat{A}=\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{\\frac{1}{2}}$, $\\tilde{A}=A+I$, $\\tilde{D}=\\sum_j \\tilde{A}_{jj}$, $\\sigma(\\cdot)=\\text{ReLU}(\\cdot)$.\n",
    "The output $Z_{vc}$ denotes the probability of assigning node $v$ to class $c$.\n",
    "\n",
    "The optimal parameters $\\theta=\\{W^{(1)}, W^{(2)}\\}$ are learned by minimizing cross-entropy on the output of the labeled samples $\\mathcal{V}_L$. After training, $Z$ denotes the class probabilities for every instance in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attack model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous node classification setting, the goal of the authors was to attack a specific <font color='red'>target node</font> $v_0\\in \\mathcal{V}$ by performing small perturbations on the graph $G(0)=(A(0), X(0))$ in order to generate a corrupted graph $G'=(A',X')$ \n",
    "where $v_0$'s prediction changes and the classification performance drops.\n",
    "\n",
    "Changes to $A(0)$ are called <font color='red'>structure attacks</font>, while changes to $X(0)$ are called <font color='red'>feature attacks</font>.\n",
    "\n",
    "The perturbations on $G(0)$ are constrained to a set of <font color='red'>attacker nodes</font> $\\mathcal{A} \\subseteq \\mathcal{V}$ and it must hold \n",
    "$$\n",
    "X′_{ui}\\neq X(0)_{ui}\\Rightarrow u\\in A \\text{, } A′_{uv}\\neq A(0)_{uv} \\Rightarrow u\\in A\\vee v \\in A\n",
    "$$\n",
    "If the target $v_0\\notin A$, we are dealing with an <font color='red'>influencer attack</font> since $v_0$ cannot be manipulated directly, but only indirectly via some influencers. If ${v_0}=A$, it is called a direct attack.\n",
    "\n",
    "They set a budget $\\Delta$ that limits the number of allowed changes:\n",
    "$$\n",
    "\\sum_u \\sum_i |X(0)_{ui}−X′_{ui}|+\\sum_{u<v}|A(0)_{uv}−A′_{uv}| \\leq \\Delta\n",
    "$$\n",
    "\n",
    "Moreover, in order to have <b>unnoticeable perturbations</b>, the authors set two more constraints:\n",
    "- Since they wanted to preserve the <b>degree distribution</b> of the graph, they generated perturbations which followed similar power-law behavior as the input and they guaranteed it through a statistical test.\n",
    "- They also managed to preserve the feature statistics through a test based on <b>feature co-occurrence</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that the authors aimed to solve the following discrete optimization problem. \n",
    "\n",
    "<b>Problem.</b> Given a graph $G(0)=(A(0),X(0))$, a target node $v_0$ and attacker nodes $\\mathcal{A}$. Let $c_{old}$ denote the class for $v_0$ based on the graph $G(0)$ (predicted or using some ground truth) and $P^{G^*_0}_{\\Delta,\\mathcal{A}}$ be the set of the graphs that respect the constraints based on the set of attackers $\\mathcal{A}$, the budget $\\Delta$ and the unnoticeable perturbations.\n",
    "Determine\n",
    "$$\n",
    "\\text{argmax}_{(A′,X′)\\in P^{G^*_0}_{\\Delta,\\mathcal{A}}} \\text{max}_{c\\neq c_{old}} \\text{ln}Z^∗_{v_0,c}−\\text{ln}Z^∗_{v_0,c_{old}}\n",
    "$$\n",
    "subject to $Z^∗=f_{\\theta^*}(A′,X′)$ with $\\theta^∗=\\text{argmin}_\\theta L(θ;A′,X′)$.\n",
    "\n",
    "<b>Note:</b>\n",
    "In this version of the optimization problem, there is a bi-level optimization problem since $\\theta^∗$ is determined based on $A'$ and $X'$ (<b> poisoning attack </b>) . As a simpler variant, one can also consider an <b>evasion attack</b> assuming the parameters are static and learned based on the old graph, $\\theta^∗=\\text{argmin}_\\theta L(\\theta;A(0),X(0))$.\n",
    "\n",
    "Since solving this problem is highly challenging due to the discreteness of the data and the large number of parameters in $\\theta$, the authors proposed a sequential approach where they first attack a surrogate model. They use this as an argument to check for transferability since they did not specifically focus on the used model but only on a surrogate one. Indeed, even if the attack is not directly related to the used model, it is effective. <br> The surrogate model is a linearization of the previous model:\n",
    "$$\n",
    "Z'=\\text{softmax}(\\hat{A}\\hat{A}XW^{(1)}W^{(2)})=\\text{softmax}(\\hat{A}^2XW)\n",
    "$$\n",
    "The authors chose to solve the new surrogate optimization problem through a greedy algorithm: at each step, they efficiently computed the scores (based on the surrogate loss) for each feasible change in structure or in features in order to find the best attack at the point. This procedure is repeated until the budget $\\Delta$ is reached. The algorithm is explained in detail in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors experimented many type of attacks (direct and influencers attacks, poisoning and evasion attacks, features and structure attacks) on different datasets. The effectiveness of the attacks is tested on different nonlinear models: GCN, Deep Walk and CNC (Column Networks for Collective Classification). The set $\\Delta=d_{v_0}+2$ where $d_{v_0}$ is the degree of the attacked node. This choice is based on the fact that node with a higher degree are more difficult to attack. <br>\n",
    "They obtained interesting results, especially in the case of direct attacks. They compared their to attacks to 2 others attacks: Fast Gradient Sign Method (FGSM) that is a direct attack on $v_0$ and RND (attack in which they modify the structure of the graph). Part of the results is shown in Table 3, please refer to the paper for the detailed experiments setting and a more complete summary of the final results. \n",
    "<img src=\"image.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of our project is to study the effectiveness of the attacks and the weakness of the model through stochastic block models. <br>\n",
    "The <b>stochastic block model</b> is a generative model for random graphs. This model tends to produce graphs containing communities, subsets characterized by being connected with one another with particular edge densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements:\n",
    "* `numpy`\n",
    "* `scipy`\n",
    "* `scikit-learn`\n",
    "* `matplotlib`\n",
    "* `tensorflow`\n",
    "* `numba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from nettack import utils, GCN\n",
    "from nettack import nettack as ntk\n",
    "import numpy as np\n",
    "gpu_id = None # set this to your desired GPU ID if you want to use GPU computations (only for the GCN/surrogate training)\n",
    "\n",
    "#our code\n",
    "#from importlib import reload \n",
    "import random\n",
    "from nettack import sbm\n",
    "import scipy.sparse as sp\n",
    "from function import avg_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load network, basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we load the data from one of the dataset present in the data folder. \n",
    "In particular, the datasets `citeseer.npz` and `cora.npz` contain information about papers. The adjacency matrices contain the information about the citations, while the features \n",
    "In the Citeseer dataset, articles are classified in 6 classes: Agents, AI, DB, IR, ML, HCI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "#_A_obs: adjacency matrix for links (structure)\n",
    "#_X_obs: features per each node  \n",
    "#_z_obs: labels per each node\n",
    "#One can also repeat the experiment with the dataset cora.npz\n",
    "_A_obs, _X_obs, _z_obs = utils.load_npz('data/citeseer.npz')\n",
    "\n",
    "#make the graph indirected, that is make the adjacency matrix _A_obs symmetric\n",
    "_A_obs = _A_obs + _A_obs.T\n",
    "_A_obs[_A_obs > 1] = 1\n",
    "\n",
    "print_matrix=False\n",
    "if(print_matrix):\n",
    "    print(_A_obs)\n",
    "    \n",
    "#we select just the largest connected component    \n",
    "lcc = utils.largest_connected_components(_A_obs)\n",
    "_A_obs = _A_obs[lcc][:,lcc]\n",
    "_X_obs = _X_obs[lcc].astype('float32')\n",
    "_z_obs = _z_obs[lcc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the random graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the matrix p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N=number of vertices        \n",
    "N=_A_obs.shape[0]\n",
    "vertices=np.array(range(0,N))\n",
    "#number of communities/labels\n",
    "num_communities=_z_obs.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector that stores the # of nodes for each community\n",
    "dim_communities=np.zeros(num_communities)\n",
    "#computes the values and fills the vector\n",
    "for i in range(num_communities):\n",
    "    dim_communities[i]=len(vertices[_z_obs==i])\n",
    "\n",
    "#upper triangular matrix of _A_obs,used to compute the ammount of links among members of \n",
    "#the same community\n",
    "temp=sp.triu(_A_obs,k=0).todense()\n",
    "#one can put norm=False is he wants to print the total number of links instead of the\n",
    "#normalized value (without the norm, the result is meaningless for the creation of the random graph) \n",
    "norm=True\n",
    "#p_hat=matrix that stores the estimated probabilities of presence of an edge that links members of \n",
    "#specified communities, that is, for each pair of communities i and j, the entry i,j of the matrix stores\n",
    "#the average value of the number of edges present in the graphs that link members of the communities i and j\n",
    "#respectively over the number of all possible edges among the 2 communities (that in the case of 2 \n",
    "#different communities is equal to dim_comm_i*dim_comm_j).\n",
    "#The average is computed also for the entries i,i that represent the probability of an edge that link \n",
    "#2 members of community i.\n",
    "p_hat=np.zeros([num_communities,num_communities])\n",
    "#for each community\n",
    "for i in range(num_communities):\n",
    "    #total number of edges that link members of community i \n",
    "    p_hat[i][i]=temp[_z_obs==i][:,_z_obs==i].sum()\n",
    "    #this value has to be normalized over the total number of possible edges that link\n",
    "    #members of the same community, that is n+dim_i(dim_i-1)/2= (dim_i)(dim_i+1)/2\n",
    "    if norm:    \n",
    "        p_hat[i][i]=2*p_hat[i][i]/(dim_communities[i]*(dim_communities[i]+1))\n",
    "    #for all the other communities\n",
    "    for j in range(i+1, num_communities):\n",
    "        #total number of edges that link a member of community i and a member of community j\n",
    "        p_hat[i][j]=_A_obs[_z_obs==i][:,_z_obs==j].sum()\n",
    "        #over all the possible links, that is dim_i*dim_j\n",
    "        if norm:\n",
    "            p_hat[i][j]=p_hat[i][j]/(dim_communities[i]*dim_communities[j])\n",
    "        p_hat[j][i]=p_hat[i][j]\n",
    "\n",
    "print_avg=False\n",
    "if(print_avg):\n",
    "    print(p_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useless part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=False\n",
    "if check:\n",
    "    community_distribution=avg_matrix(neighbors, _z_obs.max()+1, _z_obs )\n",
    "    print(community_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to change the community setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The community that is most connected is: \n",
      "0 3\n",
      "The community that is less connected is: \n",
      "1 1\n",
      "Data p_hat matrix: [[0.00794603 0.00157761 0.0011654  0.00042906 0.00057208 0.0015528 ]\n",
      " [0.00157761 0.00565093 0.00024493 0.00056838 0.00016239 0.00046983]\n",
      " [0.0011654  0.00024493 0.00613521 0.00016108 0.00072669 0.00030125]\n",
      " [0.00042906 0.00056838 0.00016108 0.00688093 0.00031534 0.00020292]\n",
      " [0.00057208 0.00016239 0.00072669 0.00031534 0.00702507 0.00134264]\n",
      " [0.0015528  0.00046983 0.00030125 0.00020292 0.00134264 0.00661959]]\n",
      "Modified matrix: [[0.00964703 0.00148271 0.00107049 0.00033416 0.00047718 0.00145789]\n",
      " [0.00148271 0.00565093 0.00024493 0.00056838 0.00016239 0.00046983]\n",
      " [0.00107049 0.00024493 0.00613521 0.00016108 0.00072669 0.00030125]\n",
      " [0.00033416 0.00056838 0.00016108 0.00688093 0.00031534 0.00020292]\n",
      " [0.00047718 0.00016239 0.00072669 0.00031534 0.00702507 0.00134264]\n",
      " [0.00145789 0.00046983 0.00030125 0.00020292 0.00134264 0.00661959]]\n"
     ]
    }
   ],
   "source": [
    "#we will call strongest_comm the community that has the highest probability to have an edges\n",
    "#that link members of the same community\n",
    "\n",
    "strongest_comm=np.argmax(np.diag(p_hat))\n",
    "temp=(p_hat-np.diag(np.diag(p_hat))).sum(axis=1)\n",
    "#print(temp)\n",
    "strongest_comm_2=np.argmax(np.diag(p_hat)-temp)\n",
    "#stongest_comm=np.argmax(np.diag(p_hat))\n",
    "#weakest_comm has the lowest probability\n",
    "weakest_comm=np.argmin(np.diag(p_hat))\n",
    "weakest_comm_2=np.argmin(np.diag(p_hat)-temp)\n",
    "print(\"The community that is most connected is: \")\n",
    "print(strongest_comm, strongest_comm_2)\n",
    "print(\"The community that is less connected is: \")\n",
    "print(weakest_comm, weakest_comm_2)\n",
    "\n",
    "\n",
    "p_hat_2=np.copy(p_hat)\n",
    "#strong=False\n",
    "#if strong:\n",
    "#    p_hat[strongest_comm][strongest_comm]=p_hat[strongest_comm][strongest_comm]*10\n",
    "#print(p_hat)\n",
    "strong=True\n",
    "n=200\n",
    "i=strongest_comm\n",
    "if strong:\n",
    "    p_hat_2[i][i]=p_hat_2[i][i]+n/sum(vertices[_z_obs==i])\n",
    "    sum_not_i=sum(vertices[_z_obs!=i])\n",
    "    for j in range(num_communities):\n",
    "        if j!=i:\n",
    "            p_hat_2[i][j]=p_hat_2[i][j]-n/sum_not_i\n",
    "            p_hat_2[j][i]=p_hat_2[i][j]\n",
    "        \n",
    "\n",
    "\n",
    "weak=False\n",
    "if weak:\n",
    "    p_hat_2[weakest_comm][weakest_comm]=p_hat_2[weakest_comm][weakest_comm]/n\n",
    "    \n",
    "print_changes=True\n",
    "if print_changes:\n",
    "    print(\"Data p_hat matrix:\",p_hat)\n",
    "    print(\"Modified matrix:\",p_hat_2)\n",
    "\n",
    "\n",
    "#_A_obs=sp.csr_matrix(sbm.SBM(N, _z_obs.max()+1, _z_obs, (community_distribution+np.eye(_z_obs.max()+1)*100)/N).block_matrix)\n",
    "#_A_obs=sp.csr_matrix(sbm.SBM(N, _z_obs.max()+1, _z_obs, community_distribution).block_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one can set n>1 if he wants to compute some generic averages over the degrees for example\n",
    "#in order to check the relationship between the random graph and the original dataset\n",
    "#for example, for n large, the average degree of the random graphs is very close to the \n",
    "#original one\n",
    "degrees_hat=np.zeros(N)\n",
    "change=False\n",
    "if change:\n",
    "    p_hat=p_hat_2\n",
    "n=1\n",
    "for i in range(n):\n",
    "    #generate a SBM based on the p_hat we just computed and the node labels/community membership\n",
    "    _A_obs_hat=sp.csr_matrix(sbm.SBM(N, _z_obs.max()+1, _z_obs, p_hat).block_matrix)\n",
    "    \n",
    "    #update/compute statistics\n",
    "    temp_neighbors=[]\n",
    "    temp_degrees=np.zeros(N)\n",
    "    for i in range(N):\n",
    "        temp_neighbors.append(_A_obs_hat[i].nonzero()[1])\n",
    "        temp_degrees[i]=len(temp_neighbors[i])\n",
    "        degrees_hat[i]+=temp_degrees[i]\n",
    "degrees_hat=degrees_hat/n    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute some statistics over the dataset in order to compare them with the random matrix that we will generate in the following part. One can decide if to print them or not through the boolean variable <i>print_stats</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS OVER THE INPUT DATA\n",
      "# nodes with 0 degree: 0\n",
      "# nodes with 1 degree: 519\n",
      "# nodes with 2 degree: 568\n",
      "# nodes with 3 degree: 358\n",
      "# nodes with degree less than average: 1445\n",
      "# nodes with degree greater than average: 42\n",
      "max degree: 99.0\n",
      "average degree: 3.5014218009478673\n",
      "degree average for community 0 : 2.991304347826087\n",
      "degree average for community 1 : 3.287257019438445\n",
      "degree average for community 2 : 3.1082474226804124\n",
      "degree average for community 3 : 2.6940789473684212\n",
      "degree average for community 4 : 4.661654135338346\n",
      "degree average for community 5 : 3.301948051948052\n",
      "\n",
      "STATISTICS OVER THE RANDOM GRAPH\n",
      "# nodes with 0 degree: 72\n",
      "# nodes with 1 degree: 236\n",
      "# nodes with 2 degree: 398\n",
      "# nodes with 3 degree: 421\n",
      "# nodes with degree less than average: 1127\n",
      "# nodes with degree greater than average: 983\n",
      "max degree: 13.0\n",
      "average degree: 3.5080568720379146\n",
      "average degree for community 0 : 2.8956521739130436\n",
      "average degree for community 1 : 3.384449244060475\n",
      "average degree for community 2 : 3.0927835051546393\n",
      "average degree for community 3 : 2.6776315789473686\n",
      "average degree for community 4 : 4.565789473684211\n",
      "average degree for community 5 : 3.438311688311688\n"
     ]
    }
   ],
   "source": [
    "neighbors=[]\n",
    "degrees=np.zeros(N)\n",
    "for i in range(N):\n",
    "    neighbors.append(_A_obs[i].nonzero()[1])\n",
    "    degrees[i]=len(neighbors[i])\n",
    "\n",
    "print_stats=True\n",
    "if(print_stats):\n",
    "    print(\"STATISTICS OVER THE INPUT DATA\")\n",
    "    for i in range(4):\n",
    "        print(\"# nodes with\",i,\"degree:\",(degrees==i).sum())\n",
    "    print(\"# nodes with degree less than average:\",(degrees<sum(degrees)/len(degrees)).sum())\n",
    "    print(\"# nodes with degree greater than average:\",(degrees>sum(degrees)/len(degrees)+10).sum())\n",
    "    print(\"max degree:\",max(degrees))\n",
    "    #print(neighbors[np.argmax(degrees)])\n",
    "    #print(np.argmax(degrees))\n",
    "    print(\"average degree:\",sum(degrees)/len(degrees))\n",
    "    for i in range(num_communities):\n",
    "        print(\"degree average for community\",i,\":\",sum(degrees[vertices[_z_obs==i]])/len(degrees[vertices[_z_obs==i]]))\n",
    "\n",
    "print_stats_rg=True\n",
    "if(print_stats_rg):\n",
    "    print(\"\\nSTATISTICS OVER THE RANDOM GRAPH\")\n",
    "    print(\"# nodes with 0 degree:\",(degrees_hat<1).sum())\n",
    "    for i in range(1,4):\n",
    "        print(\"# nodes with\",i,\"degree:\", (degrees_hat<i+1).sum()-(degrees_hat<i).sum())\n",
    "\n",
    "    print(\"# nodes with degree less than average:\",(degrees_hat<sum(degrees_hat)/len(degrees_hat)).sum())\n",
    "    j=10\n",
    "    print(\"# nodes with degree greater than average:\",(degrees_hat>sum(degrees_hat)/len(degrees_hat)).sum())\n",
    "    print(\"max degree:\",(max(degrees_hat)))\n",
    "    print(\"average degree:\",(sum(degrees_hat)/len(degrees_hat)))\n",
    "    for i in range(num_communities):\n",
    "        print(\"average degree for community\",i,\":\",sum(degrees_hat[vertices[_z_obs==i]])/len(degrees_hat[vertices[_z_obs==i]]))\n",
    "        \n",
    "#recompute a new connected component???    \n",
    "#lcc = utils.largest_connected_components(_A_obs)\n",
    "#_A_obs = _A_obs[lcc][:,lcc]\n",
    "#print(np.linalg.norm(degrees-degrees_hat))\n",
    "#print(_A_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph=False\n",
    "if random_graph:\n",
    "    _A_obs=_A_obs_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "#nettack code applied to our new dataset\n",
    "\n",
    "lcc = utils.largest_connected_components(_A_obs)\n",
    "_A_obs= _A_obs[lcc][:,lcc]\n",
    "if print_matrix: \n",
    "    print(_A_obs)\n",
    "assert np.abs(_A_obs - _A_obs.T).sum() == 0, \"Input graph is not symmetric\"\n",
    "assert _A_obs.max() == 1 and len(np.unique(_A_obs[_A_obs.nonzero()].A1)) == 1, \"Graph must be unweighted\"\n",
    "assert _A_obs.sum(0).A1.min() > 0, \"Graph contains singleton nodes\"\n",
    "\n",
    "_X_obs = _X_obs[lcc].astype('float32')\n",
    "_z_obs = _z_obs[lcc]\n",
    "_N = _A_obs.shape[0]\n",
    "_K = _z_obs.max()+1\n",
    "_Z_obs = np.eye(_K)[_z_obs]\n",
    "_An = utils.preprocess_graph(_A_obs)\n",
    "sizes = [16, _K]\n",
    "degrees = _A_obs.sum(0).A1\n",
    "neighbors=[]\n",
    "for i in range(_A_obs.shape[0]):\n",
    "    neighbors.append(_A_obs[i].nonzero()[1])\n",
    "seed = 15\n",
    "unlabeled_share = 0.8\n",
    "val_share = 0.1\n",
    "train_share = 1 - unlabeled_share - val_share\n",
    "np.random.seed(seed)\n",
    "\n",
    "split_train, split_val, split_unlabeled = utils.train_val_test_split_tabular(np.arange(_N),\n",
    "                                                                       train_size=train_share,\n",
    "                                                                       val_size=val_share,\n",
    "                                                                       test_size=unlabeled_share,\n",
    "                                                                       stratify=_z_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the node to attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981\n"
     ]
    }
   ],
   "source": [
    "u = 0 # node to attack \n",
    "vertices=np.array(range(_N))\n",
    "u=vertices[np.multiply(degrees>sum(degrees)/len(degrees),_z_obs==strongest_comm)][0]\n",
    "u=981\n",
    "#if strong:\n",
    "#    u=random.choice(vertices[np.multiply(degrees>3,_z_obs==strongest_comm)])\n",
    "#if weak:\n",
    "#    u=vertices[np.multiply(degrees>1,_z_obs==weakest_comm)][0]\n",
    "print(u)\n",
    "assert u in split_unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train surrogate model (i.e. GCN without nonlinear activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Martina\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Martina\\Desktop\\Secondo anno-Primo semestre\\Deep Learning on Computational Accelerators\\progetto\\nettack2\\nettack\\GCN.py:98: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Martina\\Desktop\\Secondo anno-Primo semestre\\Deep Learning on Computational Accelerators\\progetto\\nettack2\\nettack\\GCN.py:116: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Martina\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martina\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged after 34 iterations\n"
     ]
    }
   ],
   "source": [
    "surrogate_model = GCN.GCN(sizes, _An, _X_obs, with_relu=False, name=\"surrogate\", gpu_id=gpu_id)\n",
    "surrogate_model.train(split_train, split_val, _Z_obs)\n",
    "W1 =surrogate_model.W1.eval(session=surrogate_model.session)\n",
    "W2 =surrogate_model.W2.eval(session=surrogate_model.session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Nettack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nettack = ntk.Nettack(_A_obs, _X_obs, _z_obs, W1, W2, u, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "direct_attack = False\n",
    "n_influencers = 1 if direct_attack else 5\n",
    "strong=True\n",
    "weak=False\n",
    "if strong:\n",
    "    n_perturbations = int(degrees[u]) #int(degrees[u]/10) # How many perturbations to perform. Default: Degree of the node\n",
    "print(degrees[u])\n",
    "if weak:\n",
    "    n_perturbations=int(degrees[u]/2)\n",
    "if not strong and not weak:\n",
    "    n_perturbations=int(degrees[u]/2)\n",
    "perturb_features = False    \n",
    "perturb_structure = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poison the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Starting attack #####\n",
      "##### Attack only using structure perturbations #####\n",
      "##### Attacking the node indirectly via 5 influencer nodes #####\n",
      "##### Performing 1 perturbations #####\n",
      "(array([236], dtype=int32), array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]]))\n",
      "5\n",
      "[236]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-289d4ae2849f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnettack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnettack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattack_surrogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_perturbations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturb_structure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperturb_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturb_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperturb_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirect_attack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_influencers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_influencers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Secondo anno-Primo semestre\\Deep Learning on Computational Accelerators\\progetto\\nettack2\\nettack\\nettack.py\u001b[0m in \u001b[0;36mattack_surrogate\u001b[1;34m(self, n_perturbations, perturb_structure, perturb_features, direct, n_influencers, delta_cutoff)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_infls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfluencer_nodes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_infls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;31m# Potential edges are all edges from any attacker to any other node, except the respective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                 \u001b[1;31m# attacker itself or the node being attacked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "nettack.reset()\n",
    "nettack.attack_surrogate(n_perturbations, perturb_structure=perturb_structure, perturb_features=perturb_features, direct=direct_attack, n_influencers=n_influencers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nettack.structure_perturbations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_attack=False\n",
    "if print_attack:\n",
    "    for i in range(len(nettack.structure_perturbations)):\n",
    "        (u,attack)=nettack.structure_perturbations[i]\n",
    "        print(_z_obs[u])\n",
    "        print(_z_obs[attack])\n",
    "#print(degrees[489])\n",
    "        print(_A_obs[u,attack])\n",
    "    print(_A_obs[u])\n",
    "    for i in np.array(neighbors[u]):\n",
    "        print(_z_obs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nettack.feature_perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GCN without perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_iters=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_margins_clean = []\n",
    "class_distrs_clean = []\n",
    "gcn_before = GCN.GCN(sizes, _An, _X_obs, \"gcn_orig\", gpu_id=gpu_id)\n",
    "for _ in range(retrain_iters):\n",
    "    print(\"... {}/{} \".format(_+1, retrain_iters))\n",
    "    gcn_before.train(split_train, split_val, _Z_obs)\n",
    "    probs_before_attack = gcn_before.predictions.eval(session=gcn_before.session,feed_dict={gcn_before.node_ids: [nettack.u]})[0]\n",
    "    class_distrs_clean.append(probs_before_attack)\n",
    "    best_second_class_before = (probs_before_attack - 1000*_Z_obs[nettack.u]).argmax()\n",
    "    margin_before = probs_before_attack[_z_obs[nettack.u]] - probs_before_attack[best_second_class_before]\n",
    "    classification_margins_clean.append(margin_before)\n",
    "class_distrs_clean = np.array(class_distrs_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GCN with perturbations\n",
    "(insert your favorite node classification algorithm here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_margins_corrupted = []\n",
    "class_distrs_retrain = []\n",
    "gcn_retrain = GCN.GCN(sizes, nettack.adj_preprocessed, nettack.X_obs.tocsr(), \"gcn_retrain\", gpu_id=gpu_id)\n",
    "for _ in range(retrain_iters):\n",
    "    print(\"... {}/{} \".format(_+1, retrain_iters))\n",
    "    gcn_retrain.train(split_train, split_val, _Z_obs)\n",
    "    probs_after_attack = gcn_retrain.predictions.eval(session=gcn_retrain.session,feed_dict={gcn_retrain.node_ids: [nettack.u]})[0]\n",
    "    best_second_class_after = (probs_after_attack - 1000*_Z_obs[nettack.u]).argmax()\n",
    "    margin_after = probs_after_attack[_z_obs[nettack.u]] - probs_after_attack[best_second_class_after]\n",
    "    class_distrs_retrain.append(probs_after_attack)\n",
    "    classification_margins_corrupted.append(margin_after)\n",
    "class_distrs_retrain = np.array(class_distrs_retrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xlabel(ix, correct):\n",
    "    if ix==correct:\n",
    "        return \"Class {}\\n(correct)\".format(ix)\n",
    "    return \"Class {}\".format(ix)\n",
    "\n",
    "figure = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "center_ixs_clean = []\n",
    "for ix, block in enumerate(class_distrs_clean.T):\n",
    "    x_ixs= np.arange(len(block)) + ix*(len(block)+2)\n",
    "    center_ixs_clean.append(np.mean(x_ixs))\n",
    "    color = '#555555'\n",
    "    if ix == nettack.label_u:\n",
    "        color = 'darkgreen'\n",
    "    plt.bar(x_ixs, block, color=color)\n",
    "\n",
    "ax=plt.gca()\n",
    "plt.ylim((-.05, 1.05))\n",
    "plt.ylabel(\"Predicted probability\")\n",
    "ax.set_xticks(center_ixs_clean)\n",
    "ax.set_xticklabels([make_xlabel(k, nettack.label_u) for k in range(_K)])\n",
    "ax.set_title(\"Predicted class probabilities for node {} on clean data\\n({} re-trainings)\".format(nettack.u, retrain_iters))\n",
    "\n",
    "fig = plt.subplot(1, 2, 2)\n",
    "center_ixs_retrain = []\n",
    "for ix, block in enumerate(class_distrs_retrain.T):\n",
    "    x_ixs= np.arange(len(block)) + ix*(len(block)+2)\n",
    "    center_ixs_retrain.append(np.mean(x_ixs))\n",
    "    color = '#555555'\n",
    "    if ix == nettack.label_u:\n",
    "        color = 'darkgreen'\n",
    "    plt.bar(x_ixs, block, color=color)\n",
    "\n",
    "\n",
    "ax=plt.gca()\n",
    "plt.ylim((-.05, 1.05))\n",
    "ax.set_xticks(center_ixs_retrain)\n",
    "ax.set_xticklabels([make_xlabel(k, nettack.label_u) for k in range(_K)])\n",
    "ax.set_title(\"Predicted class probabilities for node {} after {} perturbations\\n({} re-trainings)\".format(nettack.u, n_perturbations, retrain_iters))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
